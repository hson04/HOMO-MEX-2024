{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf265d9-f4ba-4aa6-b217-2ac543d556eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch,os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48cb6f0e-b21a-4b51-967f-26be260e672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Llama-2\"\n",
    "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31853dc4-2979-4735-834c-93b4ec6a56ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_number = \"1\"\n",
    "#max_length_input=196\n",
    "lr = 2e-4\n",
    "num_epochs = 2\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce7711-79cf-440a-9c01-cd63d326000b",
   "metadata": {},
   "source": [
    "# Model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc57661c-2383-4c7f-b656-62d46d72d966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itp1hc/.conda/envs/thinSA/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-28 03:55:52.489372: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-28 03:55:52.489470: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-28 03:55:52.519559: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-28 03:55:52.599350: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-28 03:55:55.155282: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.47s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoTokenizer, GenerationConfig, TextStreamer, pipeline, AutoModelForSeq2SeqLM,LlamaTokenizerFast\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,cache_dir=\"Thin/llms\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,torch_dtype=torch.bfloat16,\n",
    "                                              device_map=\"auto\",cache_dir=\"Thin/llms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3417d684-160d-41be-99e2-c1cfb507fc7a",
   "metadata": {},
   "source": [
    "# Run experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d3720-f1d9-47c2-9b44-5f612b8963ea",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e57251a-f7a0-412b-b94f-a35b9cc395b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Golden Gay: hombres que nunca se han acostado ...</td>\n",
       "      <td>NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>#CuandoMiMamaDice Quien es ese gay que este al...</td>\n",
       "      <td>NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@RobinHil ¡Felicidades! Ganaste un pase doble ...</td>\n",
       "      <td>NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@GArnauda @LarraldeDario Ricardo del Real (hom...</td>\n",
       "      <td>NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Los conceptos no pueden ser transgénero porque...</td>\n",
       "      <td>NP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            content label\n",
       "0           0  Golden Gay: hombres que nunca se han acostado ...    NP\n",
       "1           1  #CuandoMiMamaDice Quien es ese gay que este al...    NP\n",
       "2           2  @RobinHil ¡Felicidades! Ganaste un pase doble ...    NP\n",
       "3           3  @GArnauda @LarraldeDario Ricardo del Real (hom...    NP\n",
       "4           4  Los conceptos no pueden ser transgénero porque...    NP"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_train = 'dataset/Homomexican/track_1_train.csv'\n",
    "path_dev = 'dataset/Homomexican/track_1_dev.csv'\n",
    "path_test = 'dataset/Homomexican/track_1_public_test.csv'\n",
    "path_output = \"./Output/SA/HSA/\"\n",
    "\n",
    "import pandas as pd\n",
    "data_train = pd.read_csv(path_train)\n",
    "data_train.reset_index(drop=True, inplace=True)\n",
    "#id_train = data_train['id_EXIST'].tolist()\n",
    "\n",
    "data_test= pd.read_csv(path_test)\n",
    "data_test.reset_index(drop=True, inplace=True)\n",
    "id_test = data_test[\"sub_id\"].tolist()\n",
    "\n",
    "data_dev= pd.read_csv(path_dev)\n",
    "data_dev.reset_index(drop=True, inplace=True)\n",
    "#id_dev = data_dev[\"id_EXIST\"].tolist()\n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427cde57-b3ad-45fa-87cb-511cd80022da",
   "metadata": {},
   "source": [
    "# Preprocessing tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77e97b56-207e-4c3d-a532-5718e08214a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize_hashtag(doc):\n",
    "    new_text = re.sub(r\"#\\w+\\s*\", \"\", doc, flags=re.MULTILINE) # first remove all hashtag and followed word in tweet\n",
    "    pattern = r\"(?=[A-Z 0-9])\"\n",
    "    for hashtag in re.findall(r\"(?:\\#)\\S+\", doc): #for each hashtag in text\n",
    "        if not hashtag.isupper(): # if that hashtag is not fully upper case\n",
    "            split = re.split(pattern, hashtag) #split that hashtag by upper case letter, example: GamerGirl -> Gamer Girl\n",
    "            hashtag = ' '.join(str(x) for x in split if len(str(x))>1) #some bad case: AMDRedTeam -> A M D Red Team. The only thing we could do is removing any single letter in that text\n",
    "        new_word = re.sub(r'#',\"\", hashtag, flags=re.MULTILINE) #remove \"#\" in text\n",
    "        new_text = f\"{new_text} {new_word}\" # adding processed hashtag back to tweet\n",
    "    return new_text\n",
    "\n",
    "data_train['content'] = data_train['content'].apply(normalize_hashtag)\n",
    "data_dev['content'] = data_dev['content'].apply(normalize_hashtag)\n",
    "data_test['content'] = data_test['content'].apply(normalize_hashtag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a33e83e-15d9-454e-9b42-86b9c52ea172",
   "metadata": {},
   "source": [
    "# preprocessing label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cd8c7d0-0796-41ea-81c6-38af4963b08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train['label'] = data[data.columns[1:]].values.tolist()\n",
    "\n",
    "# def id2label(id):\n",
    "#     label = ['LES','GAY','BI','TRAN','OTHER','NOT RELATED']\n",
    "#     out_label = []\n",
    "#     for index, i in enumerate(id):\n",
    "#         if i == 1:\n",
    "#             out_label.append(label[index])\n",
    "#     return ', '.join(out_label)\n",
    "\n",
    "# data_train['label'] = data_train['label'].apply(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c25115e-beb8-4a95-a6c1-47f9b4114bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instruction + output prompt contain all task\n",
    "import re\n",
    "def create_instruction_input_output(df):\n",
    "    input_text = []\n",
    "    output_text = []\n",
    "    inputs = df[\"content\"].tolist()\n",
    "    output = df[\"label\"].tolist()\n",
    "    for index, item in enumerate(inputs):     \n",
    "        item = re.sub(r\"\\s+\", \" \", item) #replace all type of white space by a single space   \n",
    "        \n",
    "        prompt_new = f\"\"\"[INST]\n",
    "Classify the sentiment of a tweet: \"{item}\"\n",
    "## if the tweet directed against any person whose sexual orientation and/or gender identity differs form cis-heterosexuality, output \"P\"\n",
    "## if the tweet not include any hate speech against the LGBT+ population but do mention this comunity, output \"NP\".\n",
    "## if the tweet not related in any way to the LGBT+ comunity, ouput \"NR\". OUTPUT only P, NP, or NR.\n",
    "Answer: [/INST]\"\"\"     \n",
    "        input_text.append(prompt_new)\n",
    "        output_text.append(output[index])\n",
    "    print(len(input_text),len(output_text))\n",
    "    return input_text,output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b13f14f4-12c9-428b-a3dd-1086771674bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8800 8800\n",
      "[INST]\n",
      "Classify the sentiment of a tweet: \"@GArnauda @LarraldeDario Ricardo del Real (hombre trans, de paso. Gran proponente de la diversidad) es parte del comité olímpico, me dice que no da no topa con el Dario. Lleva preguntando un rato por allá de hecho.\"\n",
      "## if the tweet directed against any person whose sexual orientation and/or gender identity differs form cis-heterosexuality, output \"P\"\n",
      "## if the tweet not include any hate speech against the LGBT+ population but do mention this comunity, output \"NP\".\n",
      "## if the tweet not related in any way to the LGBT+ comunity, ouput \"NR\". OUTPUT only P, NP, or NR.\n",
      "Answer: [/INST]\n",
      "NP\n"
     ]
    }
   ],
   "source": [
    "# Create instruction input ouput for each task\n",
    "input_train,output_train = create_instruction_input_output(data_train)\n",
    "print(input_train[3])\n",
    "print(output_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caa656ff-a092-4db9-9f5c-78d131e273c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[INST]\\nClassify the sentiment of a tweet: \"Go...</td>\n",
       "      <td>NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[INST]\\nClassify the sentiment of a tweet: \"Qu...</td>\n",
       "      <td>NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[INST]\\nClassify the sentiment of a tweet: \"@R...</td>\n",
       "      <td>NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[INST]\\nClassify the sentiment of a tweet: \"@G...</td>\n",
       "      <td>NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[INST]\\nClassify the sentiment of a tweet: \"Lo...</td>\n",
       "      <td>NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[INST]\\nClassify the sentiment of a tweet: \"@c...</td>\n",
       "      <td>NP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  [INST]\\nClassify the sentiment of a tweet: \"Go...    NP\n",
       "1  [INST]\\nClassify the sentiment of a tweet: \"Qu...    NP\n",
       "2  [INST]\\nClassify the sentiment of a tweet: \"@R...    NP\n",
       "3  [INST]\\nClassify the sentiment of a tweet: \"@G...    NP\n",
       "4  [INST]\\nClassify the sentiment of a tweet: \"Lo...    NP\n",
       "5  [INST]\\nClassify the sentiment of a tweet: \"@c...    NP"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(list(zip(input_train, output_train)), columns =['text', 'label'])\n",
    "# # removing rows that contain \"UNKNOWN\" OR \"\"-\"\"\n",
    "# train_df = train_df[(train_df[\"label\"] != \"-\") & (train_df[\"label\"] != \"UNKNOWN\")]\n",
    "train_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12bca27c-abc5-4262-aa81-4e701732a27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 8800\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary datatype contain train, val set\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "d1s = Dataset.from_pandas(train_df)\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = d1s\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00821f08-be87-4c60-920a-c55f347fb476",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c233631-4a71-4afa-96d9-2e602df97c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cbad82e-f9ac-45c3-b370-e51199c00588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91f5a58d-15ad-4f87-a6f9-bc6d14361eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 19988480 || all params: 6758404096 || trainable%: 0.2957573965106688\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8, # Lora attention dimension.\n",
    "        lora_alpha=16, # the alpha parameter for Lora scaling.\n",
    "        lora_dropout=0.05, # the dropout probability for Lora layers.\n",
    "        bias=\"none\",\n",
    "        target_modules=[\n",
    "            #\"embed_tokens\",\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "            #\"lm_head\",\n",
    "        ]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91ac15d9-2863-4327-933b-f596327dce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "# data preprocessing\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# max_length = max([len(tokenizer(review)[\"input_ids\"]) for review in df1[\"x_input\"][100:200].tolist()])\n",
    "# print(max_length)\n",
    "max_length = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b913db7a-7c6b-45b7-8f47-f72224e866fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset: 100%|██████████| 8800/8800 [00:02<00:00, 3406.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    batch_size = len(examples[\"text\"])\n",
    "    inputs = [item + \" \" for item in examples[\"text\"]]\n",
    "    targets = examples[\"label\"]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n",
    "        \n",
    "        # print(i, sample_input_ids, label_input_ids)\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "    # print(model_inputs)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (max_length - len(sample_input_ids)) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\"attention_mask\"][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        \n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "processed_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[f\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "#train_dataset = processed_datasets[\"train\"]\n",
    "train_dataset = processed_datasets[f\"train\"]\n",
    "#eval_dataset = processed_datasets[\"test\"]\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "#eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d337fcf8-4e10-433e-8c1b-ff12136e2c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2200/2200 [22:15<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_ppl=tensor(1.1754, device='cuda:0') train_epoch_loss=tensor(0.1616, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2200/2200 [22:13<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1: train_ppl=tensor(1.0859, device='cuda:0') train_epoch_loss=tensor(0.0824, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# optimizer and lr scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")\n",
    "\n",
    "# training and evaluation\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        #         print(batch)\n",
    "        #         print(batch[\"input_ids\"].shape)\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # model.eval()\n",
    "    # eval_loss = 0\n",
    "    # eval_preds = []\n",
    "#     for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**batch)\n",
    "#         loss = outputs.loss\n",
    "#         eval_loss += loss.detach().float()\n",
    "#         eval_preds.extend(\n",
    "#             tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
    "#         )\n",
    "\n",
    "#     eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "#     eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=}\") #{eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0952f17-e823-4481-94fc-7ff472e113b2",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d2ede17-dd41-40e1-b3f0-163b734dd7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(sample,max_len_input=max_length):\n",
    "    inputs = tokenizer(sample, return_tensors=\"pt\").to('cuda')\n",
    "    generation_config  = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    max_new_tokens=128,\n",
    "    top_k=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    penalty_alpha = 0.6,\n",
    "    return_full_text=False,\n",
    "    )\n",
    "    \n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "    output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    output = output[0].split(\"[/INST]\")[1].strip()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69099a5b-257f-4849-8ecf-109ec54220d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instruction + output prompt contain all task\n",
    "import re\n",
    "def create_instruction_input(df):\n",
    "    input_text = []\n",
    "    output_text = []\n",
    "    inputs = df[\"content\"].tolist()\n",
    "    for index, item in enumerate(inputs):     \n",
    "        item = re.sub(r\"\\s+\", \" \", item) #replace all type of white space by a single space   \n",
    "        \n",
    "        prompt_new = f\"\"\"[INST]\n",
    "Classify the sentiment of a tweet: \"{item}\"\n",
    "## if the tweet directed against any person whose sexual orientation and/or gender identity differs form cis-heterosexuality, output \"P\"\n",
    "## if the tweet not include any hate speech against the LGBT+ population but do mention this comunity, output \"NP\".\n",
    "## if the tweet not related in any way to the LGBT+ comunity, ouput \"NR\". OUTPUT only P, NP, or NR.\n",
    "Answer: [/INST]\"\"\"     \n",
    "        input_text.append(prompt_new)\n",
    "    print(len(input_text))\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c1211-c7fb-4367-82dc-78431e5754c9",
   "metadata": {},
   "source": [
    "## PREDICTION ON TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72e63ac8-7007-4667-9a5f-1f2d25f617c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200\n"
     ]
    }
   ],
   "source": [
    "test_input = create_instruction_input(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a3af110-32c3-454e-b629-dde4132f04fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2200/2200 [06:40<00:00,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200\n",
      "['NP', 'NP', 'NP', 'NP', 'NP']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "#predict one batch of datadev at a time\n",
    "for sample in tqdm(test_input):\n",
    "    pred = evaluate_model(sample)\n",
    "    y_pred.append(pred)\n",
    "\n",
    "print(len(y_pred))\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2385a4ed-261a-4618-8912-9c61334844cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_Track1</td>\n",
       "      <td>NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_Track1</td>\n",
       "      <td>NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_Track1</td>\n",
       "      <td>NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3_Track1</td>\n",
       "      <td>NP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4_Track1</td>\n",
       "      <td>NP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sub_id label\n",
       "0  0_Track1    NP\n",
       "1  1_Track1    NP\n",
       "2  2_Track1    NP\n",
       "3  3_Track1    NP\n",
       "4  4_Track1    NP"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame({'sub_id': id_test,\n",
    "                      'label': y_pred,})\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae871b8a-6a46-491c-9522-8b24e1352043",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(f'homomex11test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777ec00c-1650-4aff-8b45-2993db1e333b",
   "metadata": {},
   "source": [
    "## PREDICTION ON DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89a3e262-84ab-420f-8b6e-eca18bfab18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n"
     ]
    }
   ],
   "source": [
    "dev_input = create_instruction_input(data_dev)\n",
    "dev_output = data_dev[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e4127fa-90cd-4705-8aae-6cfd54d3007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [20:53<00:00,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "['NP', 'P', 'NP', 'P', 'P']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_dev = []\n",
    "\n",
    "#predict one batch of datadev at a time\n",
    "for sample in tqdm(dev_input):\n",
    "    pred = evaluate_model(sample)\n",
    "    y_pred_dev.append(pred)\n",
    "\n",
    "print(len(y_pred_dev))\n",
    "print(y_pred_dev[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75d85234-95c0-446e-84e8-808fe17d3be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'Homomex11dev.txt', 'w') as f: \n",
    "    for word in y_pred_dev: \n",
    "        f.write(f'{word}\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44890e4a-64f5-4ab3-8d7f-c134136cca9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e90dae1-d255-4f39-9385-2fb30f9b014d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "841 4294\n",
      "5135\n"
     ]
    }
   ],
   "source": [
    "print(y_pred_dev.count(\"P\"), y_pred_dev.count(\"NP\"))\n",
    "print(y_pred_dev.count(\"P\") + y_pred_dev.count(\"NP\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bafbd4-89fc-41ea-80c2-cfd11bf7dd07",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfba997b-c613-4114-92e6-4a401c8df87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NP       0.97      0.96      0.97      4360\n",
      "          NR       0.94      0.99      0.96      1778\n",
      "           P       0.89      0.87      0.88       862\n",
      "\n",
      "    accuracy                           0.95      7000\n",
      "   macro avg       0.93      0.94      0.94      7000\n",
      "weighted avg       0.95      0.95      0.95      7000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "print(classification_report(data_dev[\"label\"].tolist(), y_pred_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf3182d-5a74-4a68-8599-5f505690854f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thinSA",
   "language": "python",
   "name": "thinsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
