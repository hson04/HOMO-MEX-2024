{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8212188,"sourceType":"datasetVersion","datasetId":4689763}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re, string\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-20T06:44:53.501275Z","iopub.execute_input":"2024-05-20T06:44:53.502430Z","iopub.status.idle":"2024-05-20T06:44:53.514673Z","shell.execute_reply.started":"2024-05-20T06:44:53.502384Z","shell.execute_reply":"2024-05-20T06:44:53.513463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random, torch\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T06:44:53.516595Z","iopub.execute_input":"2024-05-20T06:44:53.516996Z","iopub.status.idle":"2024-05-20T06:44:53.524062Z","shell.execute_reply.started":"2024-05-20T06:44:53.516958Z","shell.execute_reply":"2024-05-20T06:44:53.523142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-05-20T06:44:53.525292Z","iopub.execute_input":"2024-05-20T06:44:53.525645Z","iopub.status.idle":"2024-05-20T06:44:53.537638Z","shell.execute_reply.started":"2024-05-20T06:44:53.525618Z","shell.execute_reply":"2024-05-20T06:44:53.536652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev_path = '/kaggle/input/homomex24-development/track_1_dev.csv'\ntrain_path = '/kaggle/input/homomex24-development/public_data_train_phase/track_1_train.csv'","metadata":{"execution":{"iopub.status.busy":"2024-05-20T06:44:53.539740Z","iopub.execute_input":"2024-05-20T06:44:53.540067Z","iopub.status.idle":"2024-05-20T06:44:53.547915Z","shell.execute_reply.started":"2024-05-20T06:44:53.540041Z","shell.execute_reply":"2024-05-20T06:44:53.546517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read data","metadata":{}},{"cell_type":"code","source":"import string,re\nimport emoji\n\ndef preprocessing_text(text):\n    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text, flags=re.MULTILINE)\n    text = text.strip()\n    text = text.translate(text.maketrans('', '', string.punctuation.replace(\"_\",\"\")))\n    text = re.sub('\\\\s+',' ',text).strip() #remove white space\n    return text\n\ndef emoji_preprocess(data, column='content'):\n    for index, row in data.iterrows():\n        data.loc[index, column] = emoji.demojize(row['content'], language='es')\n\ndef convert_label(label):\n    if label == 'NP':\n        return 0\n    elif label == 'P':\n        return 1\n    elif label == 'NR':\n        return 2\n    else:\n        print('error:', label)\n        return None\n    \ndef preprocessing_data(df):\n    df[\"content\"] = df[\"content\"].apply(preprocessing_text)\n    df[\"label\"] = df[\"label\"].apply(convert_label)\n    emoji_preprocess(df)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T06:44:53.549129Z","iopub.execute_input":"2024-05-20T06:44:53.549520Z","iopub.status.idle":"2024-05-20T06:44:53.561659Z","shell.execute_reply.started":"2024-05-20T06:44:53.549466Z","shell.execute_reply":"2024-05-20T06:44:53.560609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(train_path)\npreprocessing_data(train_data)\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T06:44:53.562853Z","iopub.execute_input":"2024-05-20T06:44:53.563200Z","iopub.status.idle":"2024-05-20T06:44:58.194223Z","shell.execute_reply.started":"2024-05-20T06:44:53.563167Z","shell.execute_reply":"2024-05-20T06:44:58.193231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(dev_path)\npreprocessing_data(test_data)\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T06:44:58.195412Z","iopub.execute_input":"2024-05-20T06:44:58.195710Z","iopub.status.idle":"2024-05-20T06:45:01.855849Z","shell.execute_reply.started":"2024-05-20T06:44:58.195684Z","shell.execute_reply":"2024-05-20T06:45:01.854788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Classifier","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel_name = \"FacebookAI/xlm-roberta-base\"\n\nbert_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\ntokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T06:45:01.856976Z","iopub.execute_input":"2024-05-20T06:45:01.857275Z","iopub.status.idle":"2024-05-20T06:45:04.172624Z","shell.execute_reply.started":"2024-05-20T06:45:01.857249Z","shell.execute_reply":"2024-05-20T06:45:04.171677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T06:45:04.173811Z","iopub.execute_input":"2024-05-20T06:45:04.174128Z","iopub.status.idle":"2024-05-20T06:45:04.181595Z","shell.execute_reply.started":"2024-05-20T06:45:04.174101Z","shell.execute_reply":"2024-05-20T06:45:04.180467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\nfrom transformers import Trainer, TrainingArguments\n\nprint(train_data.head())\npredicted_targets =[]\nactual_targets = []\ny_train= train_data['label'].tolist()\ny_test= test_data['label'].tolist()\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n#train_data = pd.DataFrame(zip(X_train,y_train),  columns=[\"text\",\"label\"])\n#test_data = pd.DataFrame(zip(X_test,y_test),  columns=[\"text\",\"label\"])\n\nprint(\"=============================================================\")\nprint(\"Train\")\nprint(train_data[\"label\"].value_counts())\nprint(\"Test\")\nprint(test_data[\"label\"].value_counts())\nprint(\"=============================================================\")\n    \ntrain_encodings = tokenizer(train_data['content'].tolist(), truncation=True, padding=True)\ntest_encodings = tokenizer(test_data['content'].tolist(), truncation=True, padding=True)\n\ntrain_dataset = CustomDataset(train_encodings, y_train)\ntest_dataset = CustomDataset(test_encodings, y_test)\n\ntraining_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    save_strategy = \"epoch\",\n    save_total_limit=1,\n    num_train_epochs=10,              # total number of training epochs\n#     evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,  # batch size per device during training\n    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n    logging_steps=100,\n    report_to='tensorboard'\n    )\n\ntrainer = Trainer(\n    model=bert_model,                    # the instantiated ðŸ¤— Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n#     eval_dataset = test_dataset\n)\ntrainer.train()\n\npreds = trainer.predict(test_dataset)[0]\ny_pred = np.argmax(preds, axis=1).flatten().tolist()\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-05-20T06:45:04.184353Z","iopub.execute_input":"2024-05-20T06:45:04.184683Z","iopub.status.idle":"2024-05-20T08:22:00.591420Z","shell.execute_reply.started":"2024-05-20T06:45:04.184655Z","shell.execute_reply":"2024-05-20T08:22:00.590381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}